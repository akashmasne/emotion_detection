{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import image as keras_image\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_data=pd.read_csv('fer2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "for index, row in emotion_data.iterrows():\n",
    "    k = row['pixels'].split(\" \")\n",
    "    if row['Usage'] == 'Training':\n",
    "        X_train.append(np.array(k))\n",
    "        y_train.append(row['emotion'])\n",
    "    elif row['Usage'] == 'PublicTest':\n",
    "        X_test.append(np.array(k))\n",
    "        y_test.append(row['emotion'])\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)\n",
    "\n",
    "y_train= np_utils.to_categorical(y_train, num_classes=7)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(ZeroPadding2D((1,1),input_shape=(48,48,1)))\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(64, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(128, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(256, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "model.add(ZeroPadding2D((1,1)))\n",
    "model.add(Convolution2D(512, 3, 3, activation='relu'))\n",
    "model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(7, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6023 samples, validate on 2935 samples\n",
      "Epoch 1/10\n",
      "6023/6023 [==============================] - 996s 165ms/step - loss: 13.6781 - acc: 0.1486 - val_loss: 14.0697 - val_acc: 0.1271\n",
      "Epoch 2/10\n",
      "6023/6023 [==============================] - 959s 159ms/step - loss: 13.7899 - acc: 0.1444 - val_loss: 14.0697 - val_acc: 0.1271\n",
      "Epoch 3/10\n",
      "6023/6023 [==============================] - 1113s 185ms/step - loss: 13.7417 - acc: 0.1474 - val_loss: 14.0697 - val_acc: 0.1271\n",
      "Epoch 4/10\n",
      "6023/6023 [==============================] - 1141s 189ms/step - loss: 13.7391 - acc: 0.1476 - val_loss: 14.0697 - val_acc: 0.1271\n",
      "Epoch 5/10\n",
      "6023/6023 [==============================] - 953s 158ms/step - loss: 13.8060 - acc: 0.1435 - val_loss: 14.0697 - val_acc: 0.1271\n",
      "Epoch 6/10\n",
      "6023/6023 [==============================] - 867s 144ms/step - loss: 13.7471 - acc: 0.1471 - val_loss: 14.0697 - val_acc: 0.1271\n",
      "Epoch 7/10\n",
      "6023/6023 [==============================] - 870s 144ms/step - loss: 13.7364 - acc: 0.1478 - val_loss: 14.0697 - val_acc: 0.1271\n",
      "Epoch 8/10\n",
      "6023/6023 [==============================] - 893s 148ms/step - loss: 13.7792 - acc: 0.1451 - val_loss: 14.0697 - val_acc: 0.1271\n",
      "Epoch 9/10\n",
      "6023/6023 [==============================] - 923s 153ms/step - loss: 13.6240 - acc: 0.1547 - val_loss: 14.0697 - val_acc: 0.1271\n",
      "Epoch 10/10\n",
      "6023/6023 [==============================] - 914s 152ms/step - loss: 13.7551 - acc: 0.1466 - val_loss: 14.0697 - val_acc: 0.1271\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1afd3d636a0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer='sgd',loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model.fit(X_train,y_train,batch_size=32,epochs=100,verbose=1,validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2935/2935 [==============================] - 90s 31ms/step\n",
      "[14.069696845553397, 0.12708688246584426]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss_and_metrics = model.evaluate(X_test,y_test)\n",
    "print(loss_and_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is saved\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    model.save_weights(\"model.h5\")\n",
    "print(\"Model is saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start webcam & test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys, getopt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import model_from_json\n",
    "from keras.preprocessing import image as keras_image\n",
    "from keras.utils import np_utils\n",
    "from keras.models import load_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Just disables the warning, doesn't enable AVX/FMA\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "model = model_from_json(open(\"./ready/fer.json\", \"r\").read())\n",
    "model.load_weights('./ready/fer.h5')\n",
    "#model=load_model('model.h5')\n",
    "\n",
    "\n",
    "# openCV provides this xml data file to detect front face\n",
    "#face_haar_cascade = cv2.CascadeClassifier('C:\\\\MyFolder\\\\Upgrad_pre_notebook\\\\emotion_detection\\\\haarcascade_frontalface_default.xml')\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.samples.findFile('haarcascade_frontalface_default.xml'))\n",
    "#face_haar_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "while (True):\n",
    "    _ret, frame=cap.read()\n",
    "    vis = frame.copy() # visual as output image\n",
    "        \n",
    "    grayFrame= cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #grayFrame = cv2.equalizeHist(grayFrame)\n",
    "    faces_detected = face_haar_cascade.detectMultiScale(grayFrame, 1.3  , 10)\n",
    "    \n",
    "    blue_color = (255, 0, 0)# Blue color in rectangle boundry\n",
    "    green_color= (0, 255, 0)\n",
    "    thickness = 2\n",
    "    font=1\n",
    "    \n",
    "    for (x,y,w,h) in faces_detected:\n",
    "        # Draw a rectangle with blue line borders of thickness of 2 px \n",
    "        cv2.rectangle(vis,(x,y), (x+w,y+h), blue_color,thickness)\n",
    "        roi_gray=grayFrame[y:y+w,x:x+h]\n",
    "        roi_gray=cv2.resize(roi_gray,(48,48))\n",
    "\n",
    "        image_pixels = keras_image.img_to_array(roi_gray)   # image from keras.preprocessing\n",
    "        image_pixels = np.expand_dims(image_pixels, axis = 0)\n",
    "        image_pixels /= 255\n",
    "    \n",
    "    predictions = model.predict(image_pixels)\n",
    "    max_index = np.argmax(predictions[0])\n",
    "    \n",
    "\n",
    "    emotion_detection = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')\n",
    "    emotion_prediction = emotion_detection[max_index]\n",
    "\n",
    "    label_position=(int(x), int(y))\n",
    "    label=emotion_prediction\n",
    "\n",
    "    cv2.putText(vis, label, label_position,cv2.FONT_HERSHEY_SIMPLEX,font,green_color,thickness,cv2.LINE_AA)\n",
    "    #cv2.putText(image,label,label_position,cv2.FONT_HERSHEY_SIMPLEX,2,(0,255,0),3)  \n",
    "    \n",
    "    resized_image = cv2.resize(vis, (1000, 700))\n",
    "\n",
    "    cv2.imshow('Detecting Emotion',vis)\n",
    "    if cv2.waitKey(15) == ord('q'): # press key q to exit\n",
    "        break\n",
    "\n",
    "print('Done')\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Reference:-\n",
    "https://medium.com/themlblog/how-to-do-facial-emotion-recognition-using-a-cnn-b7bbae79cd8f\n",
    "https://github.com/gitshanks/fer2013/blob/master/fertrain.py\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import model_from_json\n",
    "import numpy\n",
    "from time import sleep\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "#loading the model\n",
    "json_file = open('./ready/fer.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"./ready/fer.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "#setting image resizing parameters\n",
    "WIDTH = 48\n",
    "HEIGHT = 48\n",
    "x=None\n",
    "y=None\n",
    "labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "blue_color = (255, 0, 0)# Blue color in rectangle boundry\n",
    "green_color= (0, 255, 0)\n",
    "thickness = 2\n",
    "font=1\n",
    "face_haar_cascade = cv2.CascadeClassifier(cv2.samples.findFile('haarcascade_frontalface_default.xml'))\n",
    "\n",
    "#loading image\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "while (True):\n",
    "    _ret, frame=cap.read()\n",
    "    full_size_image = frame.copy() # visual as outp\n",
    "    gray=cv2.cvtColor(full_size_image,cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    faces_detected = face_haar_cascade.detectMultiScale(gray, 1.3  , 10)\n",
    "\n",
    "    #detecting faces\n",
    "    for (x, y, w, h) in faces_detected:\n",
    "            roi_gray = gray[y:y + h, x:x + w]\n",
    "            cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray, (48, 48)), -1), 0)\n",
    "            cv2.normalize(cropped_img, cropped_img, alpha=0, beta=1, norm_type=cv2.NORM_L2, dtype=cv2.CV_32F)\n",
    "            cv2.rectangle(full_size_image, (x, y), (x + w, y + h), green_color, thickness)\n",
    "            #predicting the emotion\n",
    "            yhat= loaded_model.predict(cropped_img)\n",
    "            cv2.putText(full_size_image, labels[int(np.argmax(yhat))], (x, y), cv2.FONT_HERSHEY_SIMPLEX, 0.8, blue_color, thickness, cv2.LINE_AA)\n",
    "            #print(\"Emotion: \"+labels[int(np.argmax(yhat))])\n",
    "            \n",
    "\n",
    "    resized_image = cv2.resize(full_size_image, (1000, 700))\n",
    "\n",
    "    cv2.imshow('Detecting Emotion',resized_image)\n",
    "    sleep(0.30)\n",
    "    if cv2.waitKey(30) == ord('q'): # press key q to exit\n",
    "        break\n",
    "\n",
    "print('Done')\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colour to grey image sample\n",
    "import cv2\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "while(True):\n",
    "      \n",
    "    ret, frame = capture.read()\n",
    "    grayFrame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    " \n",
    "    cv2.imshow('video gray', grayFrame)\n",
    "    cv2.imshow('video original', frame)\n",
    "      \n",
    "    if cv2.waitKey(1) == 27:  #ESC key press sends 27 \n",
    "        break\n",
    "        \n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c10eb90e199b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m#print(__doc__)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-c10eb90e199b>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m##nested_fn  = args.get('--nested-cascade', \"data/haarcascades/haarcascade_eye.xml\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m##'C:\\\\MyFolder\\\\Upgrad_pre_notebook\\\\emotion_detection\\\\haarcascade_frontalface_default.xml'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mcascade\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCascadeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'haarcascade_frontalface_default.xml'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[1;31m#nested = cv.CascadeClassifier(cv.samples.findFile(nested_fn))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv' is not defined"
     ]
    }
   ],
   "source": [
    "#Sample from opencv github\n",
    "import cv2 as cv\n",
    "\n",
    "from video import create_capture\n",
    "from common import clock, draw_str\n",
    "\n",
    "def detect(img, cascade):\n",
    "    rects = cascade.detectMultiScale(img, scaleFactor=1.3, minNeighbors=4, minSize=(30, 30),\n",
    "                                     flags=cv.CASCADE_SCALE_IMAGE)\n",
    "    if len(rects) == 0:\n",
    "        return []\n",
    "    rects[:,2:] += rects[:,:2]\n",
    "    return rects\n",
    "\n",
    "def draw_rects(img, rects, color):\n",
    "    for x1, y1, x2, y2 in rects:\n",
    "        cv.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "\n",
    "def main():\n",
    "    import sys, getopt\n",
    "    video_src = 0\n",
    "    ##nested_fn  = args.get('--nested-cascade', \"data/haarcascades/haarcascade_eye.xml\")\n",
    "    ##'C:\\\\MyFolder\\\\Upgrad_pre_notebook\\\\emotion_detection\\\\haarcascade_frontalface_default.xml'\n",
    "    cascade = cv.CascadeClassifier(cv.samples.findFile('haarcascade_frontalface_default.xml'))\n",
    "    #nested = cv.CascadeClassifier(cv.samples.findFile(nested_fn))\n",
    "\n",
    "    #cam = create_capture(video_src, fallback='synth:bg={}:noise=0.05'.format(cv.samples.findFile('lena.jpg')))\n",
    "    cam = cv.VideoCapture(0)\n",
    "    \n",
    "    while True:\n",
    "        _ret, img = cam.read()\n",
    "        gray = cv.cvtColor(img, cv.COLOR_BGR2GRAY)\n",
    "        gray = cv.equalizeHist(gray)\n",
    "\n",
    "        t = clock()\n",
    "        rects = detect(gray, cascade)\n",
    "        vis = img.copy()\n",
    "        draw_rects(vis, rects, (0, 255, 0))\n",
    "\n",
    "        for x1, y1, x2, y2 in rects:\n",
    "            roi = gray[y1:y2, x1:x2]\n",
    "            vis_roi = vis[y1:y2, x1:x2]\n",
    "            #subrects = detect(roi.copy(), nested)\n",
    "            #draw_rects(vis_roi, subrects, (255, 0, 0))\n",
    "        dt = clock() - t\n",
    "\n",
    "        draw_str(vis, (20, 20), 'time: %.1f ms' % (dt*1000))\n",
    "        cv.imshow('facedetect', vis)\n",
    "\n",
    "        if cv.waitKey(5) == 27:d\n",
    "            break\n",
    "\n",
    "    print('Done')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    #print(__doc__)\n",
    "    main()\n",
    "    cv.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
